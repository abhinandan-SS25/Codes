---
author: "Abhinandan Saha"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE, error = TRUE, fig.height = 3)
library(tidyverse)
library(kableExtra)
library(broman)
source("../../scripts/viridis.R")
source("../../scripts/ggprob.R")
theme_set(theme_minimal())
```

\newcommand{\E}{\mathsf{E}}
\newcommand{\Var}{\mathsf{Var}}
\newcommand{\SD}{\mathsf{SD}}
\renewcommand{\prob}{\mathsf{P}}

## Assignment 8

#### Due Friday, April 7, 11:59 PM CT

### Preliminaries

- Directories
    - COURSE/homework/
    - COURSE/homework/hw08/
    - COURSE/data/
    - COURSE/scripts/
- Files
  - COURSE/homework/hw08/hw08.Rmd
  - COURSE/scripts/viridis.R
  - COURSE/scripts/ggprob.R

### Aims

- Practice the normal distribution and the central limit theorem


## Problems

  1. Let $X \sim \text{Normal}(200, 40)$
so $\mu = 200$ and $\sigma = 40$.

- Find and display the values $x_1$ and $x_2$ such that:
  - $x_1 < \mu < x_2$;
  - $x_1$ and $x_2$ are equidistant from $\mu$ ($\mu - x_1 = x_2 - \mu$);
  - The area under the density between $x_1$ and $x_2$ equals 0.8 ($\prob(x_1 < X < x_2) = 0.8$).
- Create a graph showing the normal density with the area between $x_1$ and $x_2$ being shaded.

```{r}
pnorm(qnorm(0.9, 200, 40), 200, 40) - pnorm(qnorm(0.1, 200, 40), 200, 40)
gnorm(200, 40) +
  geom_norm_fill(200, 40, a = qnorm(0.9, 200, 40), b = qnorm(0.1, 200, 40))
x1 = qnorm(0.9, 200, 40)
x2 = qnorm(0.1, 200, 40)
x1
x2
```



  2. Create a small data frame with variables:
  
- `p` equal to the values 0.9, 0.95, 0.975, 0.99, and 0.995;
- `x` equal to the `p` quantile of the $\text{Normal}(400, 20)$ distribution;
- `z` equal to the *z-score* of `x`, $z = (x - \mu)/\sigma$.

- Print this full table.

```{r}
p = c(0.9, 0.95, 0.975, 0.99, 0.995)
x = qnorm(p, 400, 20)
z = (x - 400)/20
df = data.frame(p, x, z)
print(df)
```

- If the values of $\mu$ and $\sigma$ were changed and you repeated the problem for the same values of `p`, which of the columns `x` and `z` would change and which would remain the same? Briefly explain.

> The values in the x column would change but z would not. This is because, the vlaues in x depend on the value of mu and sigma inthe normal distribution. But z-score are standardized according to the percentiles and depends on the value of the data point relative to the mean and standard deviation, so since the vlaues in p doesnt chnage, z values doesn't change.






  3. Suppose that $X \sim \text{Normal}(200, 40)$.
  
  
- What is $\prob(180 < X < 250)$?
Create a graph which lets you visualize this probability.
```{r}
pnorm(250, 200, 40) - pnorm(180, 200, 40)
gnorm(200, 40) +
  geom_norm_fill(200, 40, a = 250, b = 180)
```

- What is the 0.05 quantile of this distribution?
Create a graph which lets you visualize this quantile.

```{r}
qnorm(0.05, 200, 40)
gnorm(200, 40) +
  geom_norm_fill(200, 40, a = qnorm(0.05, 200, 40), b = 0)
```


  4. Heights in a population of American adult males are approximately normal with a mean of 70 inches and a standard deviation of 3 inches.
  
- What proportion of American adult males are taller than two meters tall? (One meter equals 39.37 inches.)
- What is the 95th percentile of American adult male height?

```{r}
pnorm(2*39.37, 70, 3, lower.tail = FALSE)
qnorm(0.95, 70, 3)
```





  5. The following code chunk graphs the probabilities of the $\text{Binomial}(50, 0.37)$ distribution with bars of width one centered at the possible values of the random variable
from the 0.001 to the 0.999 quantiles of the distribution.
Bars at the 0.90 quantile and to the right are filled in a dark red color ("firebrick") and other values have the bars filled in gray.

```{r}
n = 50
p = 0.37

prob5_df = tibble(
  x = seq(qbinom(0.001, n, p), qbinom(0.999, n, p), 1),
  prob = dbinom(x, n, p)
)

plot5 = ggplot(prob5_df, aes(x = x, y = prob)) +
  geom_col(width = 1, color = "black", fill = "lightgray") +
  geom_col(width = 1, color = "black", fill = "firebrick",
           data = prob5_df %>% filter(x >= qbinom(0.90, n, p))) +
  geom_hline(yintercept = 0) +
  xlab("x") +
  ylab("Probability") +
  ggtitle("Binomial(50, 0.37) Distribution")

plot5
```

- Create and display a small data frame with the following summary statistics of this distribution:
    - `mu`, equal to the mean;
    - `sigma`, equal to the standard deviation;
    - `a_1`, equal to the 0.90 quantile;
    - `z_1`, the z-score of `a_1` ($z = (x-\mu)/\sigma$);
    - `a_2` equal to $a_1 - 0.5$, the left endpoint of a bar of width 1 centered at `a_1`;
    - `z_2`, the z-score of `a_2`.

```{r}
mu = 50*0.37
sigma = sqrt(50*0.37*(1-0.37))
a_1 = qbinom(0.90, 50, 0.37)
z_1 = (a_1 - mu)/sigma
a_2 = a_1 - 0.5
z_2 = (a_2 - mu)/sigma
df2 = data.frame("mu" = mu, "sigma" = sigma, "a_1" = a_1, "z_1" = z_1, "a_2" = a_2, "z_2" = z_2) 
print(df2)
```


- Modify the code to make the plot above by doing the following:
  - Only display the distribution for $x \ge 20$.
  - Overlay a blue normal density where the mean and standard deviation match those of the binomial distribution
  - Fill in the area under the curve to the right of `a_1` with a partly translucent color blue (settings `fill = "blue", alpha=0.5`) so that you see a violet color where this shading overlaps the firebrick red color of the exact binomial probabilities.
  
```{r}
prob6 = prob5_df %>%
  filter(x>=20)
plot6 = ggplot(prob6, aes(x = x, y = prob)) +
  geom_col(width = 1, color = "black", fill = "lightgray") +
  geom_col(width = 1, color = "black", fill = "firebrick",
           data = prob5_df %>% filter(x >= qbinom(0.90, n, p))) +
  geom_norm_fill(50*0.37, sqrt(50*0.37*(1-0.37)), a=a_1, fill = "blue", alpha = 0.5) +
  geom_norm_density(50*0.37, sqrt(50*0.37*(1-0.37)), color = "blue")+
  geom_hline(yintercept = 0) +
  xlab("x") +
  ylab("Probability") +
  ggtitle("Binomial(50, 0.37) Distribution")
plot6 
```

- Make another plot by modifying from the previous problem, but fill the area to the right of `a_2` instead of `a_1`.

```{r}
plot7 = ggplot(prob6, aes(x = x, y = prob)) +
  geom_col(width = 1, color = "black", fill = "lightgray") +
  geom_col(width = 1, color = "black", fill = "firebrick",
           data = prob5_df %>% filter(x >= qbinom(0.90, n, p))) +
  geom_norm_fill(50*0.37, sqrt(50*0.37*(1-0.37)), a=a_2, fill = "blue", alpha = 0.5) +
  geom_norm_density(50*0.37, sqrt(50*0.37*(1-0.37)), color = "blue")+
  geom_hline(yintercept = 0) +
  xlab("x") +
  ylab("Probability") +
  ggtitle("Binomial(50, 0.37) Distribution")
plot7 
```
  
- Find:
  - the exact binomial probability $\prob(X \ge a_1)$ where $a_1$ is the 0.90 quantile. 
  - the area to the right of `a_1` under the normal density
  - the area to the right of `a_2` under the normal density
  
```{r}
1 - pbinom(a_1, 50, 0.37) + dbinom(a_1, 50, 0.37)
1 - pnorm(a_1, 50*0.37, sqrt(50*0.37*(1-0.37)))
1 - pnorm(a_2, 50*0.37, sqrt(50*0.37*(1-0.37)))

```
  
- Briefly explain why one normal area is closer to the exact binomial calculation than the other, referring to your plots.

> One normal area, namely a_2, is closer to the exact binomial calculation because the columns used to visualize the binomial has a breadth of 1 unit. The bars are also centered on values and thus results are slightly different.





  6. Suppose you are playing a coin flipping game with a friend, where you suspect the coin your friend provided is not a fair coin.  In fact, you think the probability the coin lands heads is less than 0.5.  To test this, you flip the coin 100 times and observe the coin lands heads 35 times.
  
- If you assume the coin is fair (i.e., the probability of the coin landing heads is 0.5), what is the probability of observing 35 heads or fewer?

- How small would $p$ need to be (rounded to the nearest 0.01) for the probability of observing 35 or fewer heads to be at least 0.05?

```{r}
pbinom(35, 100, 0.5)
p <- seq(0, 1, by = 0.01)
p_binom_1 <- pbinom(35, size = 100, prob = p)
p_min <- max(p[p_binom_1 >= 0.05])
pbinom(35, 100, p_min)
```

- Does it seem plausible that the coin is fair? Briefly explain.

> It doesn't seem plausible that the coin is fair since the probability to 35 heads or lesser on a fair coin is very low, around 0.01% and this clearly indicates that a situation like so is very unlikely to occur. It would need to have a probability of success of atleast 0.4 to get 32 heads is atleast 0.05. Thus it doesn't seem plausible that the coin is fair.



#### Solution





  7. Suppose that a random variable $U$ is uniformly distributed between $a = 100 - 10\sqrt{3} \doteq `r myround(100 - 10*sqrt(3), 3)`$ and $b = 100 + 10\sqrt{3} \doteq `r myround(100 + 10*sqrt(3), 3)`$.
For this distribution, $\E(U) = 100$ and $\SD(U) = 10$.

Here is a plot of the density function.

```{r}
delta = 10*sqrt(3)
a = 100 - delta
b = 100 + delta

u_dist = tibble(
  u = c(a-1, a, a, b, b, b+1),
  f = c(0, 0, 1/(b-a), 1/(b-a), 0, 0)
)

ggplot(u_dist, aes(x = u, y = f)) +
  geom_line(color = "blue") +
  geom_hline(yintercept = 0)

```

This code chunk will generate four random variables $U_1, \ldots, U_4$ from this uniform density, sort them, print the sample, and then print the mean, $\overline{U}$.

```{r}
## set the seed to keep the same values when reknitting
set.seed(2023)

u = runif(4, a, b) %>% sort()
u
mean(u)
```

The next chunk of code will repeat this random sampling process $B = 1,000,000$ times and save the sample means, using code form **purrr**.

```{r}
B = 1000000
n = 4

delta = 10*sqrt(3)
a = 100 - delta
b = 100 + delta

prob7_df = tibble(
  u_bar = map_dbl(1:B, ~mean(runif(n, a, b))))
```


- Find and display the sample mean and standard deviation (use `sd()`) of the the generated sample means from the previous simulation.

```{r}
mean(prob7_df$u_bar)
sd(prob7_df$u_bar)
```

- Recall that $\E(U) = 100$ and $\SD(U) = 10$.
- What are your best guesses for the true values of the theoretical mean ($\E(\overline{U})$) and standard deviation ($\SD(\overline{U})$) of $\overline{U}$ from random samples where the sample size is $n=4$? The true values are nice round numbers.

> The best guesses are mean of 100 and standard deviation of 5.




  8. Using the simulated data from the previous problem:
  
- Estimate the probability that $\prob(96 < \overline{U} < 104)$ by finding the proportion of simulated sample means between these two values.

```{r}
favourable = count(prob7_df %>%
  filter(u_bar > 96 & u_bar < 104))
total = count(prob7_df)
favourable / total
```

- Compare this estimate to the area under a normal density with mean and standard deviation equal to the best guess theoretical values from the previous problem.

```{r}
pnorm(104, 100, 5) - pnorm(96, 100, 5)
```

- Make the following graph:
    - Use `geom_density()` to make a density plot of the 1,000,000 sampled means. Use the settings `color = "blue", alpha = 0.5` so it is partly translucent.
    - Overlay a normal density curve with the same mean and standard deviation and settings `color = "red", alpha = 0.5`.
    - Add dashed black vertical lines at 96 and 104.
    
```{r}
plot10 = ggplot(prob7_df, aes(x = u_bar)) +
  geom_density(color = "blue", alpha = 0.5) + 
  geom_norm_density(100, 5, color = "red", alpha=0.5)+
  geom_vline(xintercept=96)+
  geom_vline(xintercept=104)+
  xlab("x") +
  ylab("Probability") +
  ggtitle("Density of sampled means and theoritical mean and standard deviation")

plot10
```
    
- Briefly explain why the normal approximation is not equal to the probability estimated by simulation with reference to the graph.

> The normal approximation assumses that the distribution is normally distributed, which might not be the case for a real world dataset. Also the number of samples is huge and the approximation works better for lesser number of samples, thus the normal approximation is not equal to the probability estimated by simulation.

- Based on the graph, for about what value of $a$ will the normal approximate probability of $\prob(100 - a < \overline{U} < 100 + a)$ (area under the red curve) be the furthest from the true probability (close to the area under the blue curve). Briefly explain why.

> a is approximately 2.5. The area under the red curve is greater than the true probability predicted by the blue graph and has the most deviation from the true peobability and thus is the furthest from the true probability.



